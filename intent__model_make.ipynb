{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgsPVJF5Yg9k",
        "outputId": "99278e5c-6a41-4137-b74b-e7a6e22d5710"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2W9QxaRcivIy"
      },
      "outputs": [],
      "source": [
        "import os, copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XWrwU7rHivIz",
        "outputId": "05feeef4-83f6-4466-8ef6-8bfcdc83f8f2"
      },
      "outputs": [],
      "source": [
        "origin_df = pd.read_csv('C:/Users/Lee_Hyo_Jae/Desktop/new_project/dataset/intent_model_dataset.csv')\n",
        "df = copy.deepcopy(origin_df)\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.drop(df[df.duplicated()].index, axis=0, inplace=True)\n",
        "df.drop(df[df['intent'] == '(언약) 위협하기'].index, axis=0, inplace=True)\n",
        "df.drop(df[df['intent'] == '(표현) 부정감정 표현하기'].index, axis=0, inplace=True)\n",
        "df.drop(df[df['intent'] == '(표현) 긍정감정 표현하기'].index, axis=0, inplace=True)\n",
        "df.drop(df[df['intent'] == '(선언/위임하기)'].index, axis=0, inplace=True)\n",
        "df.drop(df.loc[df['sentence'].str.contains('\\*')].index, axis=0, inplace=True)\n",
        "df = df.loc[df['sentence'].str.len() <= 100]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ00xoZ3RrXF",
        "outputId": "0139fc86-8c14-44c9-9d46-d18814d313ca"
      },
      "outputs": [],
      "source": [
        "# 클래스 개수\n",
        "lable_num = len(df['intent'].unique())\n",
        "lable_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "A4eCP2zgivIz",
        "outputId": "3f1a1a3c-b4a8-4286-a56c-8d922858552c"
      },
      "outputs": [],
      "source": [
        "# str -> int 라벨변경\n",
        "intet_label = list(df['intent'].unique())\n",
        "\n",
        "label_dict = {}\n",
        "\n",
        "for idx, intent_lab in enumerate(intet_label) :\n",
        "    df.loc[df['intent'] == intent_lab, 'intent'] = idx\n",
        "    label_dict[idx] = intent_lab\n",
        "\n",
        "print(label_dict)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Kiw-yqCbivI0",
        "outputId": "5a0dbef6-8d4c-49a6-8442-2f2a49587afd"
      },
      "outputs": [],
      "source": [
        "train_data = pd.DataFrame()\n",
        "test_data = pd.DataFrame()\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "for k,v in label_dict.items():\n",
        "    len_ = df.loc[df['intent'] == k,'intent'].count()\n",
        "    if len_ > 20000 :\n",
        "        temp = df.loc[df['intent'] == k].sample(n=20000).reset_index(drop=True)\n",
        "        train_data = pd.concat([train_data,temp.loc[:16000]],ignore_index=True)\n",
        "        test_data = pd.concat([test_data,temp.loc[16000:]],ignore_index=True)\n",
        "    else :\n",
        "        temp = df.loc[df['intent'] == k].reset_index(drop=True)\n",
        "        train_data = pd.concat([train_data,temp.loc[:(len_*8)/10]],ignore_index=True)\n",
        "        test_data = pd.concat([test_data,temp.loc[(len_*8)/10:]],ignore_index=True)\n",
        "\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "test_data = test_data.sample(frac=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YeFYLP-mivI0"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL0xFDtcivI0",
        "outputId": "d9a95997-8615-4eaf-98e1-9697aa078902"
      },
      "outputs": [],
      "source": [
        "max_seq_len = max(df['sentence'].apply(lambda x: len(str(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wbVNVsL4ivI1"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
        "\n",
        "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
        "        # input_id는 워드임베딩을 위한 문장의 정수인코딩\n",
        "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True, truncation=True)\n",
        "\n",
        "        # attention_mask는 실제단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스.\n",
        "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
        "\n",
        "        # token_type_id은 세그먼트 인코딩\n",
        "        token_type_id = [0] * max_seq_len\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention masklength {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length{} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojORc4IFivI1",
        "outputId": "25f9f848-2c55-4bba-f165-940291b04e40"
      },
      "outputs": [],
      "source": [
        "train_X, train_y = convert_examples_to_features(train_data['sentence'], train_data['intent'],\n",
        "                                              max_seq_len=max_seq_len, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K9jSOSwivI1",
        "outputId": "0ecb33df-51c6-43d2-840b-b61bc7f47e5f"
      },
      "outputs": [],
      "source": [
        "test_X, test_y = convert_examples_to_features(test_data['sentence'], test_data['intent'],\n",
        "                                              max_seq_len=max_seq_len, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFrRthhJivI2",
        "outputId": "456b874c-61f0-417b-833e-55e69ffc3f4f"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=lable_num, from_pt=True)\n",
        "model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r3BZ9nLivI2"
      },
      "outputs": [],
      "source": [
        "model.fit(train_X, train_y, epochs=4, batch_size=128, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W1p9gcJivI2"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_X, test_y, batch_size=64)\n",
        "print(\"test loss, test acc: \", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Y1pSHxivI2"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "    input_id = tokenizer.encode(new_sentence, max_length=max_seq_len, pad_to_max_length=True)\n",
        "\n",
        "    padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "    attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
        "    token_type_id = [0] * max_seq_len\n",
        "\n",
        "    input_ids = np.array([input_id])\n",
        "    attention_masks = np.array([attention_mask])\n",
        "    token_type_ids = np.array([token_type_id])\n",
        "\n",
        "    encoded_input = [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "    score = np.argmax(model.predict(encoded_input)[0])\n",
        "    \n",
        "    print('sentence :',new_sentence)\n",
        "    print(label_dict[score])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUicj3_uivI2"
      },
      "outputs": [],
      "source": [
        "MODEL_SAVE_PATH = os.path.join(\"intent_classifier\")\n",
        "print(MODEL_SAVE_PATH)\n",
        "\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"{MODEL_SAVE_PATH} -- Folder already exists \\n\")\n",
        "else:\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "    print(f\"{MODEL_SAVE_PATH} -- Folder create complete \\n\")\n",
        "\n",
        "# save tokenizer, model\n",
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1InHBDrNivI2"
      },
      "outputs": [],
      "source": [
        "new_sentence = input('sentence > ')\n",
        "sentiment_predict(new_sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
