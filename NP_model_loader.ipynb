{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RpLTZXo4FE15"
      },
      "outputs": [],
      "source": [
        "label_dict = {0: '긍정', 1: '부정', 2: '중립'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znbYV6OUCLIR",
        "outputId": "c3e03c5c-747f-43f9-b7c5-5861dd5ec568"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16 22:08:27.336595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 22:08:27.390381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 22:08:29.359213: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1763298512.647635    1828 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5477 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "Some layers from the model checkpoint at /home/inter/chat_MBTI/NP_classifier were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /home/inter/chat_MBTI/NP_classifier.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "Device set to use 0\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextClassificationPipeline, BertTokenizerFast, TFBertForSequenceClassification, BertTokenizer\n",
        "import os\n",
        "\n",
        "\n",
        "# MODEL_NAME = 'fine-tuned-klue-bert-base'\n",
        "# MODEL_SAVE_PATH = os.path.join(\"_model\", MODEL_NAME) # change this to your preferred location\n",
        "\n",
        "MODEL_SAVE_PATH = '/home/inter/chat_MBTI/NP_classifier'\n",
        "\n",
        "# Load Fine-tuning model\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)\n",
        "model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "text_classifier = TextClassificationPipeline(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    framework='tf',\n",
        "    top_k=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LIx4OlE4CLIS"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 100\n",
        "def sentiment_predict(new_sentence):\n",
        "    input_id = tokenizer.encode(new_sentence, max_length=max_seq_len, padding='max_length')\n",
        "    # print('input_id = tokenizer.encode(new_sentence, max_length=max_seq_len, pad_to_max_length=True) :',input_id)\n",
        "\n",
        "    padding_count = input_id.count(tokenizer.pad_token_id)\n",
        "    # print('padding_count = input_id.count(tokenizer.pad_token_id) :',padding_count)\n",
        "    attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
        "    # print('attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count :',attention_mask)\n",
        "    token_type_id = [0] * max_seq_len\n",
        "    # print('token_type_id = [0] * max_seq_len :',token_type_id)\n",
        "\n",
        "    input_ids = np.array([input_id])\n",
        "    attention_masks = np.array([attention_mask])\n",
        "    token_type_ids = np.array([token_type_id])\n",
        "\n",
        "    encoded_input = [input_ids, attention_masks, token_type_ids]\n",
        "    # print('encoded_input = [input_ids, attention_masks, token_type_ids] :',encoded_input)\n",
        "\n",
        "    score = np.argmax(model.predict(encoded_input)[0])\n",
        "    # print('model.predict(encoded_input) :',model.predict(encoded_input))\n",
        "    # print('model.predict(encoded_input)[0] :',model.predict(encoded_input)[0])\n",
        "    # print('score = np.argmax(model.predict(encoded_input)[0]) :',score)\n",
        "\n",
        "    # print('sentence :',new_sentence)\n",
        "    print(label_dict[score])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS4VLXcbCLIS",
        "outputId": "76e306df-957f-445d-f3e3-766a0d51e086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "중립\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "부정\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "긍정\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "중립\n"
          ]
        }
      ],
      "source": [
        "while True :\n",
        "  new_sentence = input('sentence > ')\n",
        "  if new_sentence == '끝':\n",
        "    break\n",
        "  sentiment_predict(new_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoC-_7_WCLIS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E9GDqlx1kwVf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def NP_predict(sentence):\n",
        "\n",
        "  temp_li = []\n",
        "  score_li = text_classifier(sentence)[0]\n",
        "  for i in score_li:\n",
        "    temp_li.append(i['score'])\n",
        "\n",
        "  return label_dict[int(score_li[temp_li.index(max(temp_li))]['label'][6:])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ROgzczu1Rt",
        "outputId": "de6917c0-e575-4c19-f75c-9e0506982bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'중립'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NP_predict('아낌없이 나눠주세요')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XockZsc_GTEM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
